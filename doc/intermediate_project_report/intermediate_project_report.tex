\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

% Change this if needed to set titlebox size.
%\setlength\titlebox{5cm}

\title{LING 575: Intermediate Project Report}

\author{Claire Jaja \\
  University of Washington \\
  Seattle, WA \\
  {\tt cjaja@uw.edu} \\\And
  Andrea Kahn \\
  University of Washington \\
  Seattle, WA \\
  {\tt amkahn@uw.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
In this paper, we propose an approach to compare the effectiveness of supposedly language-agnostic sentiment analysis techniques such as machine learning algorithms on text in different languages. In addition, we plan to compare results obtained when using resources such as sentiment lexicons that are pivoted or translated from English versus using resources that are developed in the test language.
\end{abstract}

\section{Introduction}

Sentiment analysis techniques are typically developed on English text.  However, there is a proliferation of text in other languages as well, which could benefit from sentiment analysis.  Current approaches to sentiment analysis in languages other than English often involve automatically translating the text into English, which is likely to introduce errors, as well as increasing runtimes.  Furthermore, we suspect that syntactic variation between languages means that different features are more useful for different languages.

Some techniques build on machine learning algorithms that can easily be applied to other languages.  Others rely on manually developed resources such as sentiment lexicons, which are language-specific; often lexicons in languages other than English are pivoted from English rather than developed for that language.

In this paper, we propose an approach to compare the effectiveness of supposedly language-agnostic techniques such as machine learning algorithms on text in different languages. In addition, we plan to compare results obtained when using resources such as sentiment lexicons that are pivoted or translated from English versus using resources that are developed in the test language.

\section{Related Work}


\section{Data}

For our experiments, we are using comparable corpora in multiple languages.

Firstly, we use the polarity dataset v2.0 described in \cite{pang2004}, which consists of 1000 positive and 1000 negative pre-processed reviews from IMDb.  For a comparable dataset in another language, we use CorpusCine Reviews \cite{cruzmata2011}, a collection of 3,878 movie reviews written in Spanish from the muchocine.net web page.  Each review has a rating between one (most negative) and five (most positive) stars.  For our purposes, we discard the three star reviews and classify the one and two star reviews as negative and the four and five star reviews as positive.  This leaves us with 1,274 negative reviews and 1,351 positive reviews.

Additionally, we have a corpus of 1,590 English quotations from newspaper articles annotated for polarity and a comparable corpus of 2,387 German quotations from newspaper articles; both of these were annotated based on the same annotation criteria.

\section{Approach}

We employ the Mallet toolkit to train and test polarity classifiers on these two datasets.  We plan to test a variety of classification algorithms and features, in order to determine if certain ones provide better results for one language or another.  Additionally, we plan to do a linguistic error analysis, to tease apart the impact of the language differences (in this vein, Andrea hopes to use this course as a linguistics elective, while Claire is using it as a computational linguistics elective).

\section{Results}

We have implemented a baseline system on both movie review datasets using a MaxEnt classifier and unigram bag of word features.  For each dataset, we use 90\% of the reviews for training and 10\% for testing; this gives us a training set of 900 positive and 900 negative reviews for the IMDb corpus and 1,215 positive and 1,146 negative reviews for the CorpusCine corpus.  There are 100 positive and 100 negative reviews in the IMDb test set and 136 positive and 128 negative reviews in the CorpusCine test set.  The results for this system are presented below.

\vspace{5mm}
{\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{features} & \textbf{IMDb} & \textbf{CorpusCine} \\ \hline
MaxEnt & unigram & 88.00\% & 83.71\% \\ \hline
\end{tabular}

\vspace{1mm}
\emph{Table 1: Baseline results.  Test accuracy for IMDb and CorpusCine.}
\par}

\vspace{5mm}
{\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{label} & \emph{negative} & \emph{positive} \\ \hline
\emph{negative} & 86 & 14 \\ \hline
\emph{positive} & 10 & 90 \\ \hline
\end{tabular}

\vspace{1mm}
\emph{Table 2: IMDb confusion matrix. Row = true, column = predicted.}
\par}

\vspace{5mm}
{\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{label} & \emph{negative} & \emph{positive} \\ \hline
\emph{negative} & 103 & 25 \\ \hline
\emph{positive} & 18 & 118 \\ \hline
\end{tabular}

\vspace{1mm}
\emph{Table 3: CorpusCine confusion matrix. Row = true, column = predicted.}
\par}

\section{Discussion}

Our next steps include cutting back the CorpusCine train and test sets to have the same number of reviews as the IMDb train and test sets to make the results as directly comparable as possible.  To this end, we will also compare review lengths (i.e. average, minimum, and maximum number of tokens and types).  After that, we hope to implement n-fold cross validation of our results (most likely with n=10). 

Once our datasets are fully established, we will test different classification algorithms and features, as well as a variety of external resources (e.g. polarity lexicons, both native and pivoted/translated).

\section{Conclusion}

\nocite{*}
\bibliographystyle{acl}
\bibliography{references}

%\begin{thebibliography}{}

%\end{thebibliography}

\end{document}
